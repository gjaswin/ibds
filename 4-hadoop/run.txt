---------- ALL THE BELOW CONFIG HAS ALREADY BEEN DONE, SKIP ------------
envs:

# Hadoop environment variables
export HADOOP_HOME=~/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

core-site.xml:

<property>
<name>fs.defaultFS</name>
<value>hdfs://localhost:9000</value>
</property>

hdfs-site.xml:

<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:///home/YOUR_USERNAME/hadoopdata/hdfs/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:///home/YOUR_USERNAME/hadoopdata/hdfs/datanode</value>
</property>

mapred-site.xml:

<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
    
<property>
    <name>yarn.app.mapreduce.am.env</name>
    <value>HADOOP_MAPRED_HOME=/path/to/your/hadoop/installation</value>
    <description>Environment for MR AppMaster.</description>
</property>

<property>
    <name>mapreduce.map.env</name>
    <value>HADOOP_MAPRED_HOME=/path/to/your/hadoop/installation</value>
    <description>Environment for Map tasks.</description>
</property>

<property>
    <name>mapreduce.reduce.env</name>
    <value>HADOOP_MAPRED_HOME=/path/to/your/hadoop/installation</value>
    <description>Environment for Reduce tasks.</description>
</property>

yarn-site.xml:

<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
  <description>The list of auxiliary services that the NodeManager is responsible for.</description>
</property>

<property>
  <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  <description>The class name of the auxiliary service.</description>
</property>

hadoop-env.sh:

export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

------------ EXECUTION FROM HERE -------------

In terminal:

hdfs namenode -format
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh

chmod +x mapper.py
chmod +x reducer.py

hdfs dfs -mkdir -p /user/jaswin/input
hdfs dfs -put -f sample.txt /user/jaswin/input

hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \
-files mapper.py,reducer.py \
-mapper mapper.py \
-reducer reducer.py \
-input /user/$(whoami)/input \
-output /user/$(whoami)/output_mapcount


View Output:
hdfs dfs -cat /user/$(whoami)/output_mapcount/part-00000